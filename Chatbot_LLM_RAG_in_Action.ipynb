{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with LLM and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A book and movie assistant chatbot using Python, LangChain, embedding-based retrieval strategies, and OpenAI's GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preparation of the Corpus\n",
    "- The book corpus is based on the [CMU Book Summary Corpus](https://www.kaggle.com/datasets/ymaricar/cmu-book-summary-dataset).\n",
    "- The movie corpus is based on the [CMU Movie Summary Corpus](https://www.cs.cmu.edu/~ark/personas/).\n",
    "- For simplicity, I've already processed and saved the corpus snippets into **data/BookSummaries/book.json** and **data/MovieSummaries/movie.json**. If you prefer, you can download the original corpus and parse it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Vector Space\n",
    "- Embedding-Based Retrieval: LangChain supports various retrieval strategies. If LangChain's built-in retrieval mechanisms do not mean your embedding-based retrieval requirement, you can integrate LlamaIndex or a similar tool that can create and query embeddings of your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load vector_space.py\n",
    "# vector_space.py\n",
    "import json\n",
    "from pathlib import Path\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "class VectorSpaceManager:\n",
    "    def __init__(self, embedding_manager):\n",
    "        self.embedding_manager = embedding_manager\n",
    "        self.embeddings = self.embedding_manager.get_embeddings()\n",
    "\n",
    "    def create_vector_space(self, documents):\n",
    "        vector_store = FAISS.from_documents(documents[:2], self.embeddings)\n",
    "\n",
    "        with tqdm(total=len(documents), desc=\"Creating vector space\") as pbar:\n",
    "            batch_size = 100\n",
    "            for i in range(2, len(documents), batch_size):\n",
    "                batch_documents = documents[i:i+batch_size]\n",
    "                tempt_vector_store = FAISS.from_documents(batch_documents, self.embeddings)\n",
    "                vector_store.merge_from(tempt_vector_store)\n",
    "                pbar.update(len(batch_documents))\n",
    "\n",
    "        return vector_store\n",
    "\n",
    "    def save_vector_space(self, vector_store, save_path):\n",
    "        print(f\"Saving vector space to {save_path}...\")\n",
    "        vector_store.save_local(save_path)\n",
    "        print(f\"Finished!\")\n",
    "\n",
    "    def load_vector_space(self, save_path):\n",
    "        print(f\"Lodaing vector space from {save_path}\")\n",
    "        return FAISS.load_local(save_path, self.embeddings)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, json_file_path):\n",
    "        self.json_file_path = json_file_path\n",
    "\n",
    "    def load_data(self):\n",
    "        data = json.loads(Path(self.json_file_path).read_text())\n",
    "        return data\n",
    "\n",
    "    def create_documents(self, length=None):\n",
    "        data = self.load_data()\n",
    "        if length is None:\n",
    "            length = len(data)\n",
    "        \n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=self.get_page_content(item),\n",
    "                metadata=item\n",
    "            )\n",
    "            for item in data[:length]\n",
    "        ]\n",
    "        return documents\n",
    "\n",
    "    def get_page_content(self, item):\n",
    "        raise NotImplementedError(\"Subclasses must implement get_page_content method\")\n",
    "    \n",
    "\n",
    "class BookDataLoader(DataLoader):\n",
    "    def get_page_content(self, item):\n",
    "        return f\"{item['title']} {item['author']} {item['publication_date']} {item['description']} {' '.join(item['genres'])}\"\n",
    "    \n",
    "\n",
    "class MovieDataLoader(DataLoader):\n",
    "    def get_page_content(self, item):\n",
    "        # {movie_id, title, release_date, supported_languages, movie_countries, movie_genres_list, movie_actor_list, summary}\n",
    "        # Use all the fields to create the page content\n",
    "        return f\"{item['title']} {item['release_date']} {item['summary']} {' '.join(item['movie_genres_list'])} {' '.join(item['movie_actor_list'])}\"\n",
    "\n",
    "def process_data(json_file_path, model_name, save_path, data_loader_class, length=None):\n",
    "    # Initialize the embedding manager with the chosen model\n",
    "    embedding_manager = EmbeddingManager(model_name)\n",
    "\n",
    "    # Initialize the vector space manager with the embedding manager\n",
    "    vector_space_manager = VectorSpaceManager(embedding_manager)\n",
    "\n",
    "    # Load data and create documents\n",
    "    data_loader = data_loader_class(json_file_path)\n",
    "    documents = data_loader.create_documents(length=length)\n",
    "\n",
    "    # Create and save the vector space\n",
    "    vector_store = vector_space_manager.create_vector_space(documents)\n",
    "    vector_space_manager.save_vector_space(vector_store, save_path)\n",
    "\n",
    "    # Load the vector space and perform a search\n",
    "    vector_store = vector_space_manager.load_vector_space(save_path)\n",
    "    query = \"The Hobbit\"\n",
    "    search_results = vector_store.search(query, k=2, search_type=\"similarity\")\n",
    "    print(search_results)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Book\n",
    "    json_file_path = 'data/BookSummaries/book.json'  # Replace with the actual JSON file path\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    save_path = 'data/book_vector_store'  # Replace with the actual save path\n",
    "    process_data(json_file_path, model_name, save_path, BookDataLoader, 100)\n",
    "\n",
    "    # Movie\n",
    "    json_file_path = 'data/MovieSummaries/movie.json'  # Replace with the actual JSON file path\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    save_path = 'data/movie_vector_store'  # Replace with the actual save path\n",
    "    process_data(json_file_path, model_name, save_path, MovieDataLoader, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Chatbot Logic and User Interaction\n",
    "- User Query Handling: Design the chatbot to accept user input, which could range from specific questions about books to general requests for recommendations.\n",
    "- Query to Embedding: Convert the user query into an embedding and use the retrieval strategy to find the most relevant book summaries.\n",
    "- Interaction with GPT-3.5: Send the user query and retrieved book summaries to GPT-3.5 to generate a coherent and contextually appropriate response.\n",
    "- Response Generation: Combine the LLM's output with the retrieved information to generate a final response to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicClassifier:\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initializes a TopicClassifier object.\n",
    "\n",
    "        Parameters:\n",
    "        llm (LanguageModel): The language model used for classification.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.topics = [\"movies\", \"books\", \"others\"]\n",
    "\n",
    "\n",
    "    def classify(self, query):\n",
    "        \"\"\"\n",
    "        Classifies a given query into one of the predefined topics.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to be classified.\n",
    "\n",
    "        Returns:\n",
    "        str: The classified topic.\n",
    "        \"\"\"\n",
    "        prompt = f\"Classify the following question into one of these topics: '{','.join(self.topics)}': '{query}'\"\n",
    "        response = self.llm.predict(text=prompt, max_tokens=10)\n",
    "        topic = response.strip().lower()\n",
    "        return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS  \n",
    "from langchain.agents import Tool\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "class ToolManager:\n",
    "    def __init__(self, llm, movies_vector_path, books_vector_path, embeddings):\n",
    "        self.llm = llm\n",
    "        self.movies_vector_path = movies_vector_path\n",
    "        self.books_vector_path = books_vector_path\n",
    "        self.embeddings = embeddings\n",
    "        self.tools = self._initialize_tools()\n",
    "\n",
    "    def _initialize_tools(self):\n",
    "        # Load FAISS vector stores for movies and books\n",
    "        movies_vector_store = FAISS.load_local(self.movies_vector_path, self.embeddings)\n",
    "        books_vector_store = FAISS.load_local(self.books_vector_path, self.embeddings)\n",
    "\n",
    "        # Define prompt template\n",
    "        prompt_template = \"\"\"If the context is not relevant, \n",
    "        please answer the question by using your own knowledge about the topic\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "        PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "        # Initialize RetrievalQA tools with the FAISS vector stores\n",
    "        movies_qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=movies_vector_store.as_retriever(search_kwargs={\"k\": 3}), chain_type_kwargs={\"prompt\": PROMPT})\n",
    "        books_qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=books_vector_store.as_retriever(search_kwargs={\"k\": 3}), chain_type_kwargs={\"prompt\": PROMPT})\n",
    "\n",
    "        # Return a dictionary of tools for movies and books\n",
    "        return {\n",
    "            \"movies\": Tool(name=\"MoviesTool\", func=movies_qa.run, description=\"Retrieve movie information.\"),\n",
    "            \"books\": Tool(name=\"BooksTool\", func=books_qa.run, description=\"Retrieve book information.\")\n",
    "        }\n",
    "\n",
    "    def get_tool(self, topic):\n",
    "        return self.tools.get(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import ConversationalChatAgent, AgentExecutor\n",
    "\n",
    "class ChatAgent:\n",
    "    def __init__(self, llm, tool_manager):\n",
    "        self.llm = llm\n",
    "        self.tool_manager = tool_manager\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\",input_key=\"input\", return_messages=True)\n",
    "        self.agent = ConversationalChatAgent.from_llm_and_tools(llm=self.llm, tools=list(self.tool_manager.tools.values()), system_message=\"You are a smart assistant whose main goal is to recommend amazing books and movies to users. Provide helpful, **short** and concise recommendations with a touch of fun!\")\n",
    "        self.chat_agent = AgentExecutor.from_agent_and_tools(agent=self.agent, tools=list(self.tool_manager.tools.values()), verbose=True, memory=self.memory)\n",
    "\n",
    "    def get_response(self, query, topic_classifier):\n",
    "        \"\"\"\n",
    "        Get the response from the chat agent based on the given query and topic classifier.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user's query.\n",
    "            topic_classifier (TopicClassifier): The topic classifier used to classify the query.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the response generated by the chat agent.\n",
    "                  The response is stored under the key \"answer\".\n",
    "        \"\"\"\n",
    "        topic = topic_classifier.classify(query)\n",
    "        tool_name = None if topic == \"other\" else topic.capitalize() + \"Tool\"\n",
    "\n",
    "        try:\n",
    "            response = self.chat_agent.run(input=query, tool_name=tool_name) if tool_name else self.llm.generate(prompt=query)\n",
    "        except ValueError as e:\n",
    "            response = str(e)\n",
    "\n",
    "        return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Retrieve the OpenAI API key and temperature from environment variables\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY', 'YOUR_API_KEY')\n",
    "OPENAI_TEMPERATURE = float(os.getenv('OPENAI_TEMPERATURE', '0.0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model='gpt-3.5-turbo',\n",
    "    temperature=OPENAI_TEMPERATURE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vector_space import EmbeddingManager\n",
    "\n",
    "# Initialize components\n",
    "book_vector_store_path = \"data/book_vector_store\"\n",
    "movie_vector_store_path = \"data/movie_vector_store\"\n",
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = EmbeddingManager(embeddings_model_name).get_embeddings()\n",
    "tool_manager = ToolManager(llm, movie_vector_store_path, book_vector_store_path,embeddings)\n",
    "topic_classifier = TopicClassifier(llm)\n",
    "chat_agent = ChatAgent(llm, tool_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Testing and Iteration\n",
    "- Prototype Testing: Test the chatbot with a range of queries to ensure it retrieves relevant information and that LLM generates appropriate responses.\n",
    "- Iterative Improvement: Based on testing result, refine the retrieval strategy, prompt design, and response processing to improve the chatbot's accuracy and user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready to talk! Type 'quit' to exit.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I am a smart assistant here to recommend amazing books and movies to you!\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You: hello, who are you\n",
      "Chatbot: I am a smart assistant here to recommend amazing books and movies to you!\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"BooksTool\",\n",
      "    \"action_input\": \"Children's books for 2-year-olds\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mSome popular children's books for 2-year-olds include \"Goodnight Moon\" by Margaret Wise Brown, \"Brown Bear, Brown Bear, What Do You See?\" by Bill Martin Jr. and Eric Carle, \"The Very Hungry Caterpillar\" by Eric Carle, \"Where the Wild Things Are\" by Maurice Sendak, and \"Chicka Chicka Boom Boom\" by Bill Martin Jr. and John Archambault. These books are engaging, colorful, and often have simple storylines that are perfect for young children.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Some popular children's books for 2-year-olds include 'Goodnight Moon,' 'Brown Bear, Brown Bear, What Do You See?,' 'The Very Hungry Caterpillar,' 'Where the Wild Things Are,' and 'Chicka Chicka Boom Boom.' These books are engaging, colorful, and often have simple storylines that are perfect for young children.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You: I want to read a book to my 2 years old daughter, any idea?\n",
      "Chatbot: Some popular children's books for 2-year-olds include 'Goodnight Moon,' 'Brown Bear, Brown Bear, What Do You See?,' 'The Very Hungry Caterpillar,' 'Where the Wild Things Are,' and 'Chicka Chicka Boom Boom.' These books are engaging, colorful, and often have simple storylines that are perfect for young children.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"MoviesTool\",\n",
      "    \"action_input\": \"recommendations for family-friendly movies\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSome recommendations for family-friendly movies include:\n",
      "\n",
      "1. \"Finding Nemo\" (2003) - An animated film about a clownfish searching for his son, with themes of family and friendship.\n",
      "\n",
      "2. \"Toy Story\" (1995) - A heartwarming story about the secret life of toys and the importance of friendship.\n",
      "\n",
      "3. \"The Incredibles\" (2004) - A superhero family must come together to save the world in this action-packed animated film.\n",
      "\n",
      "4. \"Mary Poppins\" (1964) - A classic musical about a magical nanny who brings joy and adventure to a family in need.\n",
      "\n",
      "5. \"E.T. the Extra-Terrestrial\" (1982) - A touching story about a boy who befriends an alien and helps him return home.\n",
      "\n",
      "6. \"The Princess Bride\" (1987) - A fairy tale adventure with humor, romance, and swashbuckling action.\n",
      "\n",
      "7. \"Paddington\" (2014) - A charming film about a lovable bear who finds a new home with a kind family in London.\n",
      "\n",
      "8. \"Frozen\" (2013) - An animated musical about two sisters, Elsa and Anna, and the power of love and sisterhood.\n",
      "\n",
      "These movies are suitable for all ages and offer positive messages and entertainment for the whole family to enjoy.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Some recommendations for family-friendly movies include: 1. 'Finding Nemo' (2003), 2. 'Toy Story' (1995), 3. 'The Incredibles' (2004), 4. 'Mary Poppins' (1964), 5. 'E.T. the Extra-Terrestrial' (1982), 6. 'The Princess Bride' (1987), 7. 'Paddington' (2014), 8. 'Frozen' (2013). These movies are suitable for all ages and offer positive messages and entertainment for the whole family to enjoy.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You: what about movies?\n",
      "Chatbot: Some recommendations for family-friendly movies include: 1. 'Finding Nemo' (2003), 2. 'Toy Story' (1995), 3. 'The Incredibles' (2004), 4. 'Mary Poppins' (1964), 5. 'E.T. the Extra-Terrestrial' (1982), 6. 'The Princess Bride' (1987), 7. 'Paddington' (2014), 8. 'Frozen' (2013). These movies are suitable for all ages and offer positive messages and entertainment for the whole family to enjoy.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"I'm glad you found the recommendations helpful! Enjoy your time with your wife on vacation!\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You: Nice, thank you!\n",
      "Chatbot: I'm glad you found the recommendations helpful! Enjoy your time with your wife on vacation!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot is ready to talk! Type 'quit' to exit.\")\n",
    "    \n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    response = chat_agent.get_response(user_input, topic_classifier)\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Chatbot: {response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
